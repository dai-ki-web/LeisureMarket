{"version":3,"sources":["index.js","ls.js","lib/entry-index.js","lib/util/disposer.js","lib/content/path.js","package.json","lib/util/hash-to-segments.js","lib/util/fix-owner.js","get.js","lib/memoization.js","lib/content/read.js","put.js","lib/content/write.js","lib/util/move-file.js","rm.js","lib/content/rm.js","verify.js","lib/verify.js","lib/util/tmp.js"],"names":[],"mappings":";;;;;;;AAAA;AACA;AACA;AACA,ACHA;ADIA,ACHA;ADIA,ACHA;ADIA,AENA,ADGA;ADIA,AENA,ADGA;ADIA,AENA,ADGA;ADIA,AENA,ACHA,AFMA;ADIA,AENA,ACHA;AHUA,AENA,ACHA;AHUA,AIZA,AFMA,ACHA;AHUA,AIZA,AFMA,ACHA;AHUA,AIZA,AFMA,ACHA;AHUA,AIZA,AFMA,ACHA,AENA;ALgBA,AIZA,AFMA,ACHA,AENA;ALgBA,AIZA,AFMA,ACHA,AENA;ALgBA,AIZA,AFMA,ACHA,AGTA,ADGA;ALgBA,AIZA,AFMA,ACHA,AGTA,ADGA;ALgBA,AIZA,AFMA,ACHA,AGTA,ADGA;ALgBA,AIZA,AFMA,ACHA,AIZA,ADGA,ADGA;ALgBA,AIZA,AFMA,ACHA,AIZA,ADGA,ADGA;ALgBA,AIZA,AFMA,ACHA,AIZA,ADGA,ADGA;AGRA,ARwBA,AIZA,AFMA,ACHA,AIZA,ADGA,ADGA;AGRA,ARwBA,AIZA,AFMA,ACHA,AIZA,ADGA,ADGA;AGRA,ARwBA,AIZA,AFMA,ACHA,AIZA,AFMA;AGRA,ARwBA,AIZA,AFMA,AOrBA,ANkBA,AIZA,AFMA;AGRA,ARwBA,AIZA,AFMA,AOrBA,ANkBA,AIZA,AFMA;AGRA,ARwBA,AIZA,AFMA,AOrBA,ANkBA,AIZA,AFMA;AGRA,ARwBA,AIZA,AMlBA,ARwBA,AOrBA,ANkBA,AIZA,AFMA;AGRA,ARwBA,AIZA,AMlBA,ARwBA,AOrBA,ANkBA,AIZA,AFMA;AGRA,ARwBA,AIZA,AMlBA,ARwBA,AOrBA,ANkBA,AIZA,AFMA;AGRA,ARwBA,AIZA,AMlBA,ARwBA,AOrBA,ANkBA,AIZA,AFMA,AMlBA;AHUA,ARwBA,AIZA,AMlBA,ARwBA,AOrBA,ANkBA,AIZA,AFMA,AMlBA;AHUA,ARwBA,AIZA,AMlBA,ARwBA,AOrBA,ANkBA,AIZA,AFMA,AMlBA;AHUA,ARwBA,AIZA,AMlBA,AENA,AV8BA,AOrBA,ANkBA,AIZA,AFMA,AMlBA;AHUA,ARwBA,AIZA,AMlBA,AENA,AV8BA,AOrBA,ANkBA,AIZA,AFMA,AMlBA;AHUA,ARwBA,AIZA,AMlBA,AENA,AV8BA,AOrBA,ANkBA,AIZA,AFMA,AMlBA;AHUA,ARwBA,AIZA,AMlBA,AENA,AV8BA,AOrBA,ANkBA,AIZA,AMlBA,ARwBA,AMlBA;AHUA,ARwBA,AIZA,AMlBA,AENA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA;AHUA,ARwBA,AIZA,AMlBA,AENA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA;AHUA,ARwBA,AU9BA,AENA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA,AGTA;ANmBA,ARwBA,AU9BA,AENA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA,AGTA;ANmBA,ARwBA,AU9BA,AENA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA,AGTA;ANmBA,ARwBA,AU9BA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA,AGTA;ANmBA,ARwBA,AU9BA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA,AGTA,AENA;ARyBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA,AGTA,AENA;ARyBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,ARwBA,AMlBA,AGTA,AENA;ARyBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA,AGTA,AENA;ARyBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AKfA,AHSA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA,AGTA;ANmBA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AKfA,ADGA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AOrBA,AFMA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AMlBA,AIZA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA,AMlBA;AHUA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA,AZoCA;AGRA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,AKfA,AU9BA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,AENA,AV8BA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;AT4BA,AENA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;APsBA,ARwBA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA,Ae7CA;Af8CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","file":"index.js","sourcesContent":["\n\nconst ls = require('./ls.js')\nconst get = require('./get.js')\nconst put = require('./put.js')\nconst rm = require('./rm.js')\nconst verify = require('./verify.js')\nconst { clearMemoized } = require('./lib/memoization.js')\nconst tmp = require('./lib/util/tmp.js')\nconst index = require('./lib/entry-index.js')\n\nmodule.exports.index = {}\nmodule.exports.index.compact = index.compact\nmodule.exports.index.insert = index.insert\n\nmodule.exports.ls = ls\nmodule.exports.ls.stream = ls.stream\n\nmodule.exports.get = get\nmodule.exports.get.byDigest = get.byDigest\nmodule.exports.get.sync = get.sync\nmodule.exports.get.sync.byDigest = get.sync.byDigest\nmodule.exports.get.stream = get.stream\nmodule.exports.get.stream.byDigest = get.stream.byDigest\nmodule.exports.get.copy = get.copy\nmodule.exports.get.copy.byDigest = get.copy.byDigest\nmodule.exports.get.info = get.info\nmodule.exports.get.hasContent = get.hasContent\nmodule.exports.get.hasContent.sync = get.hasContent.sync\n\nmodule.exports.put = put\nmodule.exports.put.stream = put.stream\n\nmodule.exports.rm = rm.entry\nmodule.exports.rm.all = rm.all\nmodule.exports.rm.entry = module.exports.rm\nmodule.exports.rm.content = rm.content\n\nmodule.exports.clearMemoized = clearMemoized\n\nmodule.exports.tmp = {}\nmodule.exports.tmp.mkdir = tmp.mkdir\nmodule.exports.tmp.withTmp = tmp.withTmp\n\nmodule.exports.verify = verify\nmodule.exports.verify.lastRun = verify.lastRun\n","\n\nconst index = require('./lib/entry-index')\n\nmodule.exports = index.ls\nmodule.exports.stream = index.lsStream\n","\n\nconst util = require('util')\nconst crypto = require('crypto')\nconst fs = require('fs')\nconst Minipass = require('minipass')\nconst path = require('path')\nconst ssri = require('ssri')\nconst uniqueFilename = require('unique-filename')\n\nconst { disposer } = require('./util/disposer')\nconst contentPath = require('./content/path')\nconst fixOwner = require('./util/fix-owner')\nconst hashToSegments = require('./util/hash-to-segments')\nconst indexV = require('../package.json')['cache-version'].index\nconst moveFile = require('@npmcli/move-file')\nconst _rimraf = require('rimraf')\nconst rimraf = util.promisify(_rimraf)\nrimraf.sync = _rimraf.sync\n\nconst appendFile = util.promisify(fs.appendFile)\nconst readFile = util.promisify(fs.readFile)\nconst readdir = util.promisify(fs.readdir)\nconst writeFile = util.promisify(fs.writeFile)\n\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor (cache, key) {\n    super(`No cache entry for ${key} found in ${cache}`)\n    this.code = 'ENOENT'\n    this.cache = cache\n    this.key = key\n  }\n}\n\nmodule.exports.compact = compact\n\nasync function compact (cache, key, matchFn, opts = {}) {\n  const bucket = bucketPath(cache, key)\n  const entries = await bucketEntries(bucket)\n  const newEntries = []\n  // we loop backwards because the bottom-most result is the newest\n  // since we add new entries with appendFile\n  for (let i = entries.length - 1; i >= 0; --i) {\n    const entry = entries[i]\n    // a null integrity could mean either a delete was appended\n    // or the user has simply stored an index that does not map\n    // to any content. we determine if the user wants to keep the\n    // null integrity based on the validateEntry function passed in options.\n    // if the integrity is null and no validateEntry is provided, we break\n    // as we consider the null integrity to be a deletion of everything\n    // that came before it.\n    if (entry.integrity === null && !opts.validateEntry)\n      break\n\n    // if this entry is valid, and it is either the first entry or\n    // the newEntries array doesn't already include an entry that\n    // matches this one based on the provided matchFn, then we add\n    // it to the beginning of our list\n    if ((!opts.validateEntry || opts.validateEntry(entry) === true) &&\n      (newEntries.length === 0 ||\n        !newEntries.find((oldEntry) => matchFn(oldEntry, entry))))\n      newEntries.unshift(entry)\n  }\n\n  const newIndex = '\\n' + newEntries.map((entry) => {\n    const stringified = JSON.stringify(entry)\n    const hash = hashEntry(stringified)\n    return `${hash}\\t${stringified}`\n  }).join('\\n')\n\n  const setup = async () => {\n    const target = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix)\n    await fixOwner.mkdirfix(cache, path.dirname(target))\n    return {\n      target,\n      moved: false,\n    }\n  }\n\n  const teardown = async (tmp) => {\n    if (!tmp.moved)\n      return rimraf(tmp.target)\n  }\n\n  const write = async (tmp) => {\n    await writeFile(tmp.target, newIndex, { flag: 'wx' })\n    await fixOwner.mkdirfix(cache, path.dirname(bucket))\n    // we use @npmcli/move-file directly here because we\n    // want to overwrite the existing file\n    await moveFile(tmp.target, bucket)\n    tmp.moved = true\n    try {\n      await fixOwner.chownr(cache, bucket)\n    } catch (err) {\n      if (err.code !== 'ENOENT')\n        throw err\n    }\n  }\n\n  // write the file atomically\n  await disposer(setup(), teardown, write)\n\n  // we reverse the list we generated such that the newest\n  // entries come first in order to make looping through them easier\n  // the true passed to formatEntry tells it to keep null\n  // integrity values, if they made it this far it's because\n  // validateEntry returned true, and as such we should return it\n  return newEntries.reverse().map((entry) => formatEntry(cache, entry, true))\n}\n\nmodule.exports.insert = insert\n\nfunction insert (cache, key, integrity, opts = {}) {\n  const { metadata, size } = opts\n  const bucket = bucketPath(cache, key)\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size,\n    metadata,\n  }\n  return fixOwner\n    .mkdirfix(cache, path.dirname(bucket))\n    .then(() => {\n      const stringified = JSON.stringify(entry)\n      // NOTE - Cleverness ahoy!\n      //\n      // This works because it's tremendously unlikely for an entry to corrupt\n      // another while still preserving the string length of the JSON in\n      // question. So, we just slap the length in there and verify it on read.\n      //\n      // Thanks to @isaacs for the whiteboarding session that ended up with\n      // this.\n      return appendFile(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`)\n    })\n    .then(() => fixOwner.chownr(cache, bucket))\n    .catch((err) => {\n      if (err.code === 'ENOENT')\n        return undefined\n\n      throw err\n      // There's a class of race conditions that happen when things get deleted\n      // during fixOwner, or between the two mkdirfix/chownr calls.\n      //\n      // It's perfectly fine to just not bother in those cases and lie\n      // that the index entry was written. Because it's a cache.\n    })\n    .then(() => {\n      return formatEntry(cache, entry)\n    })\n}\n\nmodule.exports.insert.sync = insertSync\n\nfunction insertSync (cache, key, integrity, opts = {}) {\n  const { metadata, size } = opts\n  const bucket = bucketPath(cache, key)\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size,\n    metadata,\n  }\n  fixOwner.mkdirfix.sync(cache, path.dirname(bucket))\n  const stringified = JSON.stringify(entry)\n  fs.appendFileSync(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`)\n  try {\n    fixOwner.chownr.sync(cache, bucket)\n  } catch (err) {\n    if (err.code !== 'ENOENT')\n      throw err\n  }\n  return formatEntry(cache, entry)\n}\n\nmodule.exports.find = find\n\nfunction find (cache, key) {\n  const bucket = bucketPath(cache, key)\n  return bucketEntries(bucket)\n    .then((entries) => {\n      return entries.reduce((latest, next) => {\n        if (next && next.key === key)\n          return formatEntry(cache, next)\n        else\n          return latest\n      }, null)\n    })\n    .catch((err) => {\n      if (err.code === 'ENOENT')\n        return null\n      else\n        throw err\n    })\n}\n\nmodule.exports.find.sync = findSync\n\nfunction findSync (cache, key) {\n  const bucket = bucketPath(cache, key)\n  try {\n    return bucketEntriesSync(bucket).reduce((latest, next) => {\n      if (next && next.key === key)\n        return formatEntry(cache, next)\n      else\n        return latest\n    }, null)\n  } catch (err) {\n    if (err.code === 'ENOENT')\n      return null\n    else\n      throw err\n  }\n}\n\nmodule.exports.delete = del\n\nfunction del (cache, key, opts = {}) {\n  if (!opts.removeFully)\n    return insert(cache, key, null, opts)\n\n  const bucket = bucketPath(cache, key)\n  return rimraf(bucket)\n}\n\nmodule.exports.delete.sync = delSync\n\nfunction delSync (cache, key, opts = {}) {\n  if (!opts.removeFully)\n    return insertSync(cache, key, null, opts)\n\n  const bucket = bucketPath(cache, key)\n  return rimraf.sync(bucket)\n}\n\nmodule.exports.lsStream = lsStream\n\nfunction lsStream (cache) {\n  const indexDir = bucketDir(cache)\n  const stream = new Minipass({ objectMode: true })\n\n  readdirOrEmpty(indexDir).then(buckets => Promise.all(\n    buckets.map(bucket => {\n      const bucketPath = path.join(indexDir, bucket)\n      return readdirOrEmpty(bucketPath).then(subbuckets => Promise.all(\n        subbuckets.map(subbucket => {\n          const subbucketPath = path.join(bucketPath, subbucket)\n\n          // \"/cachename/<bucket 0xFF>/<bucket 0xFF>./*\"\n          return readdirOrEmpty(subbucketPath).then(entries => Promise.all(\n            entries.map(entry => {\n              const entryPath = path.join(subbucketPath, entry)\n              return bucketEntries(entryPath).then(entries =>\n                // using a Map here prevents duplicate keys from\n                // showing up twice, I guess?\n                entries.reduce((acc, entry) => {\n                  acc.set(entry.key, entry)\n                  return acc\n                }, new Map())\n              ).then(reduced => {\n                // reduced is a map of key => entry\n                for (const entry of reduced.values()) {\n                  const formatted = formatEntry(cache, entry)\n                  if (formatted)\n                    stream.write(formatted)\n                }\n              }).catch(err => {\n                if (err.code === 'ENOENT')\n                  return undefined\n                throw err\n              })\n            })\n          ))\n        })\n      ))\n    })\n  ))\n    .then(\n      () => stream.end(),\n      err => stream.emit('error', err)\n    )\n\n  return stream\n}\n\nmodule.exports.ls = ls\n\nfunction ls (cache) {\n  return lsStream(cache).collect().then(entries =>\n    entries.reduce((acc, xs) => {\n      acc[xs.key] = xs\n      return acc\n    }, {})\n  )\n}\n\nmodule.exports.bucketEntries = bucketEntries\n\nfunction bucketEntries (bucket, filter) {\n  return readFile(bucket, 'utf8').then((data) => _bucketEntries(data, filter))\n}\n\nmodule.exports.bucketEntries.sync = bucketEntriesSync\n\nfunction bucketEntriesSync (bucket, filter) {\n  const data = fs.readFileSync(bucket, 'utf8')\n  return _bucketEntries(data, filter)\n}\n\nfunction _bucketEntries (data, filter) {\n  const entries = []\n  data.split('\\n').forEach((entry) => {\n    if (!entry)\n      return\n\n    const pieces = entry.split('\\t')\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return\n    }\n    let obj\n    try {\n      obj = JSON.parse(pieces[1])\n    } catch (e) {\n      // Entry is corrupted!\n      return\n    }\n    if (obj)\n      entries.push(obj)\n  })\n  return entries\n}\n\nmodule.exports.bucketDir = bucketDir\n\nfunction bucketDir (cache) {\n  return path.join(cache, `index-v${indexV}`)\n}\n\nmodule.exports.bucketPath = bucketPath\n\nfunction bucketPath (cache, key) {\n  const hashed = hashKey(key)\n  return path.join.apply(\n    path,\n    [bucketDir(cache)].concat(hashToSegments(hashed))\n  )\n}\n\nmodule.exports.hashKey = hashKey\n\nfunction hashKey (key) {\n  return hash(key, 'sha256')\n}\n\nmodule.exports.hashEntry = hashEntry\n\nfunction hashEntry (str) {\n  return hash(str, 'sha1')\n}\n\nfunction hash (str, digest) {\n  return crypto\n    .createHash(digest)\n    .update(str)\n    .digest('hex')\n}\n\nfunction formatEntry (cache, entry, keepAll) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity && !keepAll)\n    return null\n\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: entry.integrity ? contentPath(cache, entry.integrity) : undefined,\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata,\n  }\n}\n\nfunction readdirOrEmpty (dir) {\n  return readdir(dir).catch((err) => {\n    if (err.code === 'ENOENT' || err.code === 'ENOTDIR')\n      return []\n\n    throw err\n  })\n}\n","\n\nmodule.exports.disposer = disposer\n\nfunction disposer (creatorFn, disposerFn, fn) {\n  const runDisposer = (resource, result, shouldThrow = false) => {\n    return disposerFn(resource)\n      .then(\n        // disposer resolved, do something with original fn's promise\n        () => {\n          if (shouldThrow)\n            throw result\n\n          return result\n        },\n        // Disposer fn failed, crash process\n        (err) => {\n          throw err\n          // Or process.exit?\n        })\n  }\n\n  return creatorFn\n    .then((resource) => {\n      // fn(resource) can throw, so wrap in a promise here\n      return Promise.resolve().then(() => fn(resource))\n        .then((result) => runDisposer(resource, result))\n        .catch((err) => runDisposer(resource, err, true))\n    })\n}\n","\n\nconst contentVer = require('../../package.json')['cache-version'].content\nconst hashToSegments = require('../util/hash-to-segments')\nconst path = require('path')\nconst ssri = require('ssri')\n\n// Current format of content file path:\n//\n// sha512-BaSE64Hex= ->\n// ~/.my-cache/content-v2/sha512/ba/da/55deadbeefc0ffee\n//\nmodule.exports = contentPath\n\nfunction contentPath (cache, integrity) {\n  const sri = ssri.parse(integrity, { single: true })\n  // contentPath is the *strongest* algo given\n  return path.join(\n    contentDir(cache),\n    sri.algorithm,\n    ...hashToSegments(sri.hexDigest())\n  )\n}\n\nmodule.exports.contentDir = contentDir\n\nfunction contentDir (cache) {\n  return path.join(cache, `content-v${contentVer}`)\n}\n","module.exports = {\n  \"_from\": \"cacache@^15.2.0\",\n  \"_id\": \"cacache@15.3.0\",\n  \"_inBundle\": false,\n  \"_integrity\": \"sha1-3IU4D7L1Vv492kxxm/oOyHWn8es=\",\n  \"_location\": \"/cacache\",\n  \"_phantomChildren\": {\n    \"glob\": \"7.2.0\"\n  },\n  \"_requested\": {\n    \"type\": \"range\",\n    \"registry\": true,\n    \"raw\": \"cacache@^15.2.0\",\n    \"name\": \"cacache\",\n    \"escapedName\": \"cacache\",\n    \"rawSpec\": \"^15.2.0\",\n    \"saveSpec\": null,\n    \"fetchSpec\": \"^15.2.0\"\n  },\n  \"_requiredBy\": [\n    \"/make-fetch-happen\"\n  ],\n  \"_resolved\": \"https://registry.nlark.com/cacache/download/cacache-15.3.0.tgz\",\n  \"_shasum\": \"dc85380fb2f556fe3dda4c719bfa0ec875a7f1eb\",\n  \"_spec\": \"cacache@^15.2.0\",\n  \"_where\": \"D:\\\\毕设小程序\\\\闲者集市\\\\proj\\\\node_modules\\\\make-fetch-happen\",\n  \"bugs\": {\n    \"url\": \"https://github.com/npm/cacache/issues\"\n  },\n  \"bundleDependencies\": false,\n  \"cache-version\": {\n    \"content\": \"2\",\n    \"index\": \"5\"\n  },\n  \"dependencies\": {\n    \"@npmcli/fs\": \"^1.0.0\",\n    \"@npmcli/move-file\": \"^1.0.1\",\n    \"chownr\": \"^2.0.0\",\n    \"fs-minipass\": \"^2.0.0\",\n    \"glob\": \"^7.1.4\",\n    \"infer-owner\": \"^1.0.4\",\n    \"lru-cache\": \"^6.0.0\",\n    \"minipass\": \"^3.1.1\",\n    \"minipass-collect\": \"^1.0.2\",\n    \"minipass-flush\": \"^1.0.5\",\n    \"minipass-pipeline\": \"^1.2.2\",\n    \"mkdirp\": \"^1.0.3\",\n    \"p-map\": \"^4.0.0\",\n    \"promise-inflight\": \"^1.0.1\",\n    \"rimraf\": \"^3.0.2\",\n    \"ssri\": \"^8.0.1\",\n    \"tar\": \"^6.0.2\",\n    \"unique-filename\": \"^1.1.1\"\n  },\n  \"deprecated\": false,\n  \"description\": \"Fast, fault-tolerant, cross-platform, disk-based, data-agnostic, content-addressable cache.\",\n  \"devDependencies\": {\n    \"@npmcli/lint\": \"^1.0.1\",\n    \"benchmark\": \"^2.1.4\",\n    \"chalk\": \"^4.0.0\",\n    \"require-inject\": \"^1.4.4\",\n    \"tacks\": \"^1.3.0\",\n    \"tap\": \"^15.0.9\"\n  },\n  \"engines\": {\n    \"node\": \">= 10\"\n  },\n  \"files\": [\n    \"*.js\",\n    \"lib\"\n  ],\n  \"homepage\": \"https://github.com/npm/cacache#readme\",\n  \"keywords\": [\n    \"cache\",\n    \"caching\",\n    \"content-addressable\",\n    \"sri\",\n    \"sri hash\",\n    \"subresource integrity\",\n    \"cache\",\n    \"storage\",\n    \"store\",\n    \"file store\",\n    \"filesystem\",\n    \"disk cache\",\n    \"disk storage\"\n  ],\n  \"license\": \"ISC\",\n  \"main\": \"index.js\",\n  \"name\": \"cacache\",\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git+https://github.com/npm/cacache.git\"\n  },\n  \"scripts\": {\n    \"benchmarks\": \"node test/benchmarks\",\n    \"coverage\": \"tap\",\n    \"lint\": \"npm run npmclilint -- \\\"*.*js\\\" \\\"lib/**/*.*js\\\" \\\"test/**/*.*js\\\"\",\n    \"lintfix\": \"npm run lint -- --fix\",\n    \"npmclilint\": \"npmcli-lint\",\n    \"postsnap\": \"npm run lintfix --\",\n    \"postversion\": \"npm publish\",\n    \"prepublishOnly\": \"git push origin --follow-tags\",\n    \"preversion\": \"npm test\",\n    \"snap\": \"tap\",\n    \"test\": \"tap\",\n    \"test-docker\": \"docker run -it --rm --name pacotest -v \\\"$PWD\\\":/tmp -w /tmp node:latest npm test\"\n  },\n  \"tap\": {\n    \"100\": true,\n    \"test-regex\": \"test/[^/]*.js\"\n  },\n  \"version\": \"15.3.0\"\n}\n","\n\nmodule.exports = hashToSegments\n\nfunction hashToSegments (hash) {\n  return [hash.slice(0, 2), hash.slice(2, 4), hash.slice(4)]\n}\n","\n\nconst util = require('util')\n\nconst chownr = util.promisify(require('chownr'))\nconst mkdirp = require('mkdirp')\nconst inflight = require('promise-inflight')\nconst inferOwner = require('infer-owner')\n\n// Memoize getuid()/getgid() calls.\n// patch process.setuid/setgid to invalidate cached value on change\nconst self = { uid: null, gid: null }\nconst getSelf = () => {\n  if (typeof self.uid !== 'number') {\n    self.uid = process.getuid()\n    const setuid = process.setuid\n    process.setuid = (uid) => {\n      self.uid = null\n      process.setuid = setuid\n      return process.setuid(uid)\n    }\n  }\n  if (typeof self.gid !== 'number') {\n    self.gid = process.getgid()\n    const setgid = process.setgid\n    process.setgid = (gid) => {\n      self.gid = null\n      process.setgid = setgid\n      return process.setgid(gid)\n    }\n  }\n}\n\nmodule.exports.chownr = fixOwner\n\nfunction fixOwner (cache, filepath) {\n  if (!process.getuid) {\n    // This platform doesn't need ownership fixing\n    return Promise.resolve()\n  }\n\n  getSelf()\n  if (self.uid !== 0) {\n    // almost certainly can't chown anyway\n    return Promise.resolve()\n  }\n\n  return Promise.resolve(inferOwner(cache)).then((owner) => {\n    const { uid, gid } = owner\n\n    // No need to override if it's already what we used.\n    if (self.uid === uid && self.gid === gid)\n      return\n\n    return inflight('fixOwner: fixing ownership on ' + filepath, () =>\n      chownr(\n        filepath,\n        typeof uid === 'number' ? uid : self.uid,\n        typeof gid === 'number' ? gid : self.gid\n      ).catch((err) => {\n        if (err.code === 'ENOENT')\n          return null\n\n        throw err\n      })\n    )\n  })\n}\n\nmodule.exports.chownr.sync = fixOwnerSync\n\nfunction fixOwnerSync (cache, filepath) {\n  if (!process.getuid) {\n    // This platform doesn't need ownership fixing\n    return\n  }\n  const { uid, gid } = inferOwner.sync(cache)\n  getSelf()\n  if (self.uid !== 0) {\n    // almost certainly can't chown anyway\n    return\n  }\n\n  if (self.uid === uid && self.gid === gid) {\n    // No need to override if it's already what we used.\n    return\n  }\n  try {\n    chownr.sync(\n      filepath,\n      typeof uid === 'number' ? uid : self.uid,\n      typeof gid === 'number' ? gid : self.gid\n    )\n  } catch (err) {\n    // only catch ENOENT, any other error is a problem.\n    if (err.code === 'ENOENT')\n      return null\n\n    throw err\n  }\n}\n\nmodule.exports.mkdirfix = mkdirfix\n\nfunction mkdirfix (cache, p, cb) {\n  // we have to infer the owner _before_ making the directory, even though\n  // we aren't going to use the results, since the cache itself might not\n  // exist yet.  If we mkdirp it, then our current uid/gid will be assumed\n  // to be correct if it creates the cache folder in the process.\n  return Promise.resolve(inferOwner(cache)).then(() => {\n    return mkdirp(p)\n      .then((made) => {\n        if (made)\n          return fixOwner(cache, made).then(() => made)\n      })\n      .catch((err) => {\n        if (err.code === 'EEXIST')\n          return fixOwner(cache, p).then(() => null)\n\n        throw err\n      })\n  })\n}\n\nmodule.exports.mkdirfix.sync = mkdirfixSync\n\nfunction mkdirfixSync (cache, p) {\n  try {\n    inferOwner.sync(cache)\n    const made = mkdirp.sync(p)\n    if (made) {\n      fixOwnerSync(cache, made)\n      return made\n    }\n  } catch (err) {\n    if (err.code === 'EEXIST') {\n      fixOwnerSync(cache, p)\n      return null\n    } else\n      throw err\n  }\n}\n","\n\nconst Collect = require('minipass-collect')\nconst Minipass = require('minipass')\nconst Pipeline = require('minipass-pipeline')\nconst fs = require('fs')\nconst util = require('util')\n\nconst index = require('./lib/entry-index')\nconst memo = require('./lib/memoization')\nconst read = require('./lib/content/read')\n\nconst writeFile = util.promisify(fs.writeFile)\n\nfunction getData (cache, key, opts = {}) {\n  const { integrity, memoize, size } = opts\n  const memoized = memo.get(cache, key, opts)\n  if (memoized && memoize !== false) {\n    return Promise.resolve({\n      metadata: memoized.entry.metadata,\n      data: memoized.data,\n      integrity: memoized.entry.integrity,\n      size: memoized.entry.size,\n    })\n  }\n\n  return index.find(cache, key, opts).then((entry) => {\n    if (!entry)\n      throw new index.NotFoundError(cache, key)\n\n    return read(cache, entry.integrity, { integrity, size }).then((data) => {\n      if (memoize)\n        memo.put(cache, entry, data, opts)\n\n      return {\n        data,\n        metadata: entry.metadata,\n        size: entry.size,\n        integrity: entry.integrity,\n      }\n    })\n  })\n}\nmodule.exports = getData\n\nfunction getDataByDigest (cache, key, opts = {}) {\n  const { integrity, memoize, size } = opts\n  const memoized = memo.get.byDigest(cache, key, opts)\n  if (memoized && memoize !== false)\n    return Promise.resolve(memoized)\n\n  return read(cache, key, { integrity, size }).then((res) => {\n    if (memoize)\n      memo.put.byDigest(cache, key, res, opts)\n    return res\n  })\n}\nmodule.exports.byDigest = getDataByDigest\n\nfunction getDataSync (cache, key, opts = {}) {\n  const { integrity, memoize, size } = opts\n  const memoized = memo.get(cache, key, opts)\n\n  if (memoized && memoize !== false) {\n    return {\n      metadata: memoized.entry.metadata,\n      data: memoized.data,\n      integrity: memoized.entry.integrity,\n      size: memoized.entry.size,\n    }\n  }\n  const entry = index.find.sync(cache, key, opts)\n  if (!entry)\n    throw new index.NotFoundError(cache, key)\n  const data = read.sync(cache, entry.integrity, {\n    integrity: integrity,\n    size: size,\n  })\n  const res = {\n    metadata: entry.metadata,\n    data: data,\n    size: entry.size,\n    integrity: entry.integrity,\n  }\n  if (memoize)\n    memo.put(cache, entry, res.data, opts)\n\n  return res\n}\n\nmodule.exports.sync = getDataSync\n\nfunction getDataByDigestSync (cache, digest, opts = {}) {\n  const { integrity, memoize, size } = opts\n  const memoized = memo.get.byDigest(cache, digest, opts)\n\n  if (memoized && memoize !== false)\n    return memoized\n\n  const res = read.sync(cache, digest, {\n    integrity: integrity,\n    size: size,\n  })\n  if (memoize)\n    memo.put.byDigest(cache, digest, res, opts)\n\n  return res\n}\nmodule.exports.sync.byDigest = getDataByDigestSync\n\nconst getMemoizedStream = (memoized) => {\n  const stream = new Minipass()\n  stream.on('newListener', function (ev, cb) {\n    ev === 'metadata' && cb(memoized.entry.metadata)\n    ev === 'integrity' && cb(memoized.entry.integrity)\n    ev === 'size' && cb(memoized.entry.size)\n  })\n  stream.end(memoized.data)\n  return stream\n}\n\nfunction getStream (cache, key, opts = {}) {\n  const { memoize, size } = opts\n  const memoized = memo.get(cache, key, opts)\n  if (memoized && memoize !== false)\n    return getMemoizedStream(memoized)\n\n  const stream = new Pipeline()\n  index\n    .find(cache, key)\n    .then((entry) => {\n      if (!entry)\n        throw new index.NotFoundError(cache, key)\n\n      stream.emit('metadata', entry.metadata)\n      stream.emit('integrity', entry.integrity)\n      stream.emit('size', entry.size)\n      stream.on('newListener', function (ev, cb) {\n        ev === 'metadata' && cb(entry.metadata)\n        ev === 'integrity' && cb(entry.integrity)\n        ev === 'size' && cb(entry.size)\n      })\n\n      const src = read.readStream(\n        cache,\n        entry.integrity,\n        { ...opts, size: typeof size !== 'number' ? entry.size : size }\n      )\n\n      if (memoize) {\n        const memoStream = new Collect.PassThrough()\n        memoStream.on('collect', data => memo.put(cache, entry, data, opts))\n        stream.unshift(memoStream)\n      }\n      stream.unshift(src)\n    })\n    .catch((err) => stream.emit('error', err))\n\n  return stream\n}\n\nmodule.exports.stream = getStream\n\nfunction getStreamDigest (cache, integrity, opts = {}) {\n  const { memoize } = opts\n  const memoized = memo.get.byDigest(cache, integrity, opts)\n  if (memoized && memoize !== false) {\n    const stream = new Minipass()\n    stream.end(memoized)\n    return stream\n  } else {\n    const stream = read.readStream(cache, integrity, opts)\n    if (!memoize)\n      return stream\n\n    const memoStream = new Collect.PassThrough()\n    memoStream.on('collect', data => memo.put.byDigest(\n      cache,\n      integrity,\n      data,\n      opts\n    ))\n    return new Pipeline(stream, memoStream)\n  }\n}\n\nmodule.exports.stream.byDigest = getStreamDigest\n\nfunction info (cache, key, opts = {}) {\n  const { memoize } = opts\n  const memoized = memo.get(cache, key, opts)\n  if (memoized && memoize !== false)\n    return Promise.resolve(memoized.entry)\n  else\n    return index.find(cache, key)\n}\nmodule.exports.info = info\n\nfunction copy (cache, key, dest, opts = {}) {\n  if (read.copy) {\n    return index.find(cache, key, opts).then((entry) => {\n      if (!entry)\n        throw new index.NotFoundError(cache, key)\n      return read.copy(cache, entry.integrity, dest, opts)\n        .then(() => {\n          return {\n            metadata: entry.metadata,\n            size: entry.size,\n            integrity: entry.integrity,\n          }\n        })\n    })\n  }\n\n  return getData(cache, key, opts).then((res) => {\n    return writeFile(dest, res.data).then(() => {\n      return {\n        metadata: res.metadata,\n        size: res.size,\n        integrity: res.integrity,\n      }\n    })\n  })\n}\nmodule.exports.copy = copy\n\nfunction copyByDigest (cache, key, dest, opts = {}) {\n  if (read.copy)\n    return read.copy(cache, key, dest, opts).then(() => key)\n\n  return getDataByDigest(cache, key, opts).then((res) => {\n    return writeFile(dest, res).then(() => key)\n  })\n}\nmodule.exports.copy.byDigest = copyByDigest\n\nmodule.exports.hasContent = read.hasContent\n","\n\nconst LRU = require('lru-cache')\n\nconst MAX_SIZE = 50 * 1024 * 1024 // 50MB\nconst MAX_AGE = 3 * 60 * 1000\n\nconst MEMOIZED = new LRU({\n  max: MAX_SIZE,\n  maxAge: MAX_AGE,\n  length: (entry, key) => key.startsWith('key:') ? entry.data.length : entry.length,\n})\n\nmodule.exports.clearMemoized = clearMemoized\n\nfunction clearMemoized () {\n  const old = {}\n  MEMOIZED.forEach((v, k) => {\n    old[k] = v\n  })\n  MEMOIZED.reset()\n  return old\n}\n\nmodule.exports.put = put\n\nfunction put (cache, entry, data, opts) {\n  pickMem(opts).set(`key:${cache}:${entry.key}`, { entry, data })\n  putDigest(cache, entry.integrity, data, opts)\n}\n\nmodule.exports.put.byDigest = putDigest\n\nfunction putDigest (cache, integrity, data, opts) {\n  pickMem(opts).set(`digest:${cache}:${integrity}`, data)\n}\n\nmodule.exports.get = get\n\nfunction get (cache, key, opts) {\n  return pickMem(opts).get(`key:${cache}:${key}`)\n}\n\nmodule.exports.get.byDigest = getDigest\n\nfunction getDigest (cache, integrity, opts) {\n  return pickMem(opts).get(`digest:${cache}:${integrity}`)\n}\n\nclass ObjProxy {\n  constructor (obj) {\n    this.obj = obj\n  }\n\n  get (key) {\n    return this.obj[key]\n  }\n\n  set (key, val) {\n    this.obj[key] = val\n  }\n}\n\nfunction pickMem (opts) {\n  if (!opts || !opts.memoize)\n    return MEMOIZED\n  else if (opts.memoize.get && opts.memoize.set)\n    return opts.memoize\n  else if (typeof opts.memoize === 'object')\n    return new ObjProxy(opts.memoize)\n  else\n    return MEMOIZED\n}\n","\n\nconst util = require('util')\n\nconst fs = require('fs')\nconst fsm = require('fs-minipass')\nconst ssri = require('ssri')\nconst contentPath = require('./path')\nconst Pipeline = require('minipass-pipeline')\n\nconst lstat = util.promisify(fs.lstat)\nconst readFile = util.promisify(fs.readFile)\n\nmodule.exports = read\n\nconst MAX_SINGLE_READ_SIZE = 64 * 1024 * 1024\nfunction read (cache, integrity, opts = {}) {\n  const { size } = opts\n  return withContentSri(cache, integrity, (cpath, sri) => {\n    // get size\n    return lstat(cpath).then(stat => ({ stat, cpath, sri }))\n  }).then(({ stat, cpath, sri }) => {\n    if (typeof size === 'number' && stat.size !== size)\n      throw sizeError(size, stat.size)\n\n    if (stat.size > MAX_SINGLE_READ_SIZE)\n      return readPipeline(cpath, stat.size, sri, new Pipeline()).concat()\n\n    return readFile(cpath, null).then((data) => {\n      if (!ssri.checkData(data, sri))\n        throw integrityError(sri, cpath)\n\n      return data\n    })\n  })\n}\n\nconst readPipeline = (cpath, size, sri, stream) => {\n  stream.push(\n    new fsm.ReadStream(cpath, {\n      size,\n      readSize: MAX_SINGLE_READ_SIZE,\n    }),\n    ssri.integrityStream({\n      integrity: sri,\n      size,\n    })\n  )\n  return stream\n}\n\nmodule.exports.sync = readSync\n\nfunction readSync (cache, integrity, opts = {}) {\n  const { size } = opts\n  return withContentSriSync(cache, integrity, (cpath, sri) => {\n    const data = fs.readFileSync(cpath)\n    if (typeof size === 'number' && size !== data.length)\n      throw sizeError(size, data.length)\n\n    if (ssri.checkData(data, sri))\n      return data\n\n    throw integrityError(sri, cpath)\n  })\n}\n\nmodule.exports.stream = readStream\nmodule.exports.readStream = readStream\n\nfunction readStream (cache, integrity, opts = {}) {\n  const { size } = opts\n  const stream = new Pipeline()\n  withContentSri(cache, integrity, (cpath, sri) => {\n    // just lstat to ensure it exists\n    return lstat(cpath).then((stat) => ({ stat, cpath, sri }))\n  }).then(({ stat, cpath, sri }) => {\n    if (typeof size === 'number' && size !== stat.size)\n      return stream.emit('error', sizeError(size, stat.size))\n\n    readPipeline(cpath, stat.size, sri, stream)\n  }, er => stream.emit('error', er))\n\n  return stream\n}\n\nlet copyFile\nif (fs.copyFile) {\n  module.exports.copy = copy\n  module.exports.copy.sync = copySync\n  copyFile = util.promisify(fs.copyFile)\n}\n\nfunction copy (cache, integrity, dest) {\n  return withContentSri(cache, integrity, (cpath, sri) => {\n    return copyFile(cpath, dest)\n  })\n}\n\nfunction copySync (cache, integrity, dest) {\n  return withContentSriSync(cache, integrity, (cpath, sri) => {\n    return fs.copyFileSync(cpath, dest)\n  })\n}\n\nmodule.exports.hasContent = hasContent\n\nfunction hasContent (cache, integrity) {\n  if (!integrity)\n    return Promise.resolve(false)\n\n  return withContentSri(cache, integrity, (cpath, sri) => {\n    return lstat(cpath).then((stat) => ({ size: stat.size, sri, stat }))\n  }).catch((err) => {\n    if (err.code === 'ENOENT')\n      return false\n\n    if (err.code === 'EPERM') {\n      /* istanbul ignore else */\n      if (process.platform !== 'win32')\n        throw err\n      else\n        return false\n    }\n  })\n}\n\nmodule.exports.hasContent.sync = hasContentSync\n\nfunction hasContentSync (cache, integrity) {\n  if (!integrity)\n    return false\n\n  return withContentSriSync(cache, integrity, (cpath, sri) => {\n    try {\n      const stat = fs.lstatSync(cpath)\n      return { size: stat.size, sri, stat }\n    } catch (err) {\n      if (err.code === 'ENOENT')\n        return false\n\n      if (err.code === 'EPERM') {\n        /* istanbul ignore else */\n        if (process.platform !== 'win32')\n          throw err\n        else\n          return false\n      }\n    }\n  })\n}\n\nfunction withContentSri (cache, integrity, fn) {\n  const tryFn = () => {\n    const sri = ssri.parse(integrity)\n    // If `integrity` has multiple entries, pick the first digest\n    // with available local data.\n    const algo = sri.pickAlgorithm()\n    const digests = sri[algo]\n\n    if (digests.length <= 1) {\n      const cpath = contentPath(cache, digests[0])\n      return fn(cpath, digests[0])\n    } else {\n      // Can't use race here because a generic error can happen before\n      // a ENOENT error, and can happen before a valid result\n      return Promise\n        .all(digests.map((meta) => {\n          return withContentSri(cache, meta, fn)\n            .catch((err) => {\n              if (err.code === 'ENOENT') {\n                return Object.assign(\n                  new Error('No matching content found for ' + sri.toString()),\n                  { code: 'ENOENT' }\n                )\n              }\n              return err\n            })\n        }))\n        .then((results) => {\n          // Return the first non error if it is found\n          const result = results.find((r) => !(r instanceof Error))\n          if (result)\n            return result\n\n          // Throw the No matching content found error\n          const enoentError = results.find((r) => r.code === 'ENOENT')\n          if (enoentError)\n            throw enoentError\n\n          // Throw generic error\n          throw results.find((r) => r instanceof Error)\n        })\n    }\n  }\n\n  return new Promise((resolve, reject) => {\n    try {\n      tryFn()\n        .then(resolve)\n        .catch(reject)\n    } catch (err) {\n      reject(err)\n    }\n  })\n}\n\nfunction withContentSriSync (cache, integrity, fn) {\n  const sri = ssri.parse(integrity)\n  // If `integrity` has multiple entries, pick the first digest\n  // with available local data.\n  const algo = sri.pickAlgorithm()\n  const digests = sri[algo]\n  if (digests.length <= 1) {\n    const cpath = contentPath(cache, digests[0])\n    return fn(cpath, digests[0])\n  } else {\n    let lastErr = null\n    for (const meta of digests) {\n      try {\n        return withContentSriSync(cache, meta, fn)\n      } catch (err) {\n        lastErr = err\n      }\n    }\n    throw lastErr\n  }\n}\n\nfunction sizeError (expected, found) {\n  const err = new Error(`Bad data size: expected inserted data to be ${expected} bytes, but got ${found} instead`)\n  err.expected = expected\n  err.found = found\n  err.code = 'EBADSIZE'\n  return err\n}\n\nfunction integrityError (sri, path) {\n  const err = new Error(`Integrity verification failed for ${sri} (${path})`)\n  err.code = 'EINTEGRITY'\n  err.sri = sri\n  err.path = path\n  return err\n}\n","\n\nconst index = require('./lib/entry-index')\nconst memo = require('./lib/memoization')\nconst write = require('./lib/content/write')\nconst Flush = require('minipass-flush')\nconst { PassThrough } = require('minipass-collect')\nconst Pipeline = require('minipass-pipeline')\n\nconst putOpts = (opts) => ({\n  algorithms: ['sha512'],\n  ...opts,\n})\n\nmodule.exports = putData\n\nfunction putData (cache, key, data, opts = {}) {\n  const { memoize } = opts\n  opts = putOpts(opts)\n  return write(cache, data, opts).then((res) => {\n    return index\n      .insert(cache, key, res.integrity, { ...opts, size: res.size })\n      .then((entry) => {\n        if (memoize)\n          memo.put(cache, entry, data, opts)\n\n        return res.integrity\n      })\n  })\n}\n\nmodule.exports.stream = putStream\n\nfunction putStream (cache, key, opts = {}) {\n  const { memoize } = opts\n  opts = putOpts(opts)\n  let integrity\n  let size\n\n  let memoData\n  const pipeline = new Pipeline()\n  // first item in the pipeline is the memoizer, because we need\n  // that to end first and get the collected data.\n  if (memoize) {\n    const memoizer = new PassThrough().on('collect', data => {\n      memoData = data\n    })\n    pipeline.push(memoizer)\n  }\n\n  // contentStream is a write-only, not a passthrough\n  // no data comes out of it.\n  const contentStream = write.stream(cache, opts)\n    .on('integrity', (int) => {\n      integrity = int\n    })\n    .on('size', (s) => {\n      size = s\n    })\n\n  pipeline.push(contentStream)\n\n  // last but not least, we write the index and emit hash and size,\n  // and memoize if we're doing that\n  pipeline.push(new Flush({\n    flush () {\n      return index\n        .insert(cache, key, integrity, { ...opts, size })\n        .then((entry) => {\n          if (memoize && memoData)\n            memo.put(cache, entry, memoData, opts)\n\n          if (integrity)\n            pipeline.emit('integrity', integrity)\n\n          if (size)\n            pipeline.emit('size', size)\n        })\n    },\n  }))\n\n  return pipeline\n}\n","\n\nconst util = require('util')\n\nconst contentPath = require('./path')\nconst fixOwner = require('../util/fix-owner')\nconst fs = require('fs')\nconst moveFile = require('../util/move-file')\nconst Minipass = require('minipass')\nconst Pipeline = require('minipass-pipeline')\nconst Flush = require('minipass-flush')\nconst path = require('path')\nconst rimraf = util.promisify(require('rimraf'))\nconst ssri = require('ssri')\nconst uniqueFilename = require('unique-filename')\nconst { disposer } = require('./../util/disposer')\nconst fsm = require('fs-minipass')\n\nconst writeFile = util.promisify(fs.writeFile)\n\nmodule.exports = write\n\nfunction write (cache, data, opts = {}) {\n  const { algorithms, size, integrity } = opts\n  if (algorithms && algorithms.length > 1)\n    throw new Error('opts.algorithms only supports a single algorithm for now')\n\n  if (typeof size === 'number' && data.length !== size)\n    return Promise.reject(sizeError(size, data.length))\n\n  const sri = ssri.fromData(data, algorithms ? { algorithms } : {})\n  if (integrity && !ssri.checkData(data, integrity, opts))\n    return Promise.reject(checksumError(integrity, sri))\n\n  return disposer(makeTmp(cache, opts), makeTmpDisposer,\n    (tmp) => {\n      return writeFile(tmp.target, data, { flag: 'wx' })\n        .then(() => moveToDestination(tmp, cache, sri, opts))\n    })\n    .then(() => ({ integrity: sri, size: data.length }))\n}\n\nmodule.exports.stream = writeStream\n\n// writes proxied to the 'inputStream' that is passed to the Promise\n// 'end' is deferred until content is handled.\nclass CacacheWriteStream extends Flush {\n  constructor (cache, opts) {\n    super()\n    this.opts = opts\n    this.cache = cache\n    this.inputStream = new Minipass()\n    this.inputStream.on('error', er => this.emit('error', er))\n    this.inputStream.on('drain', () => this.emit('drain'))\n    this.handleContentP = null\n  }\n\n  write (chunk, encoding, cb) {\n    if (!this.handleContentP) {\n      this.handleContentP = handleContent(\n        this.inputStream,\n        this.cache,\n        this.opts\n      )\n    }\n    return this.inputStream.write(chunk, encoding, cb)\n  }\n\n  flush (cb) {\n    this.inputStream.end(() => {\n      if (!this.handleContentP) {\n        const e = new Error('Cache input stream was empty')\n        e.code = 'ENODATA'\n        // empty streams are probably emitting end right away.\n        // defer this one tick by rejecting a promise on it.\n        return Promise.reject(e).catch(cb)\n      }\n      this.handleContentP.then(\n        (res) => {\n          res.integrity && this.emit('integrity', res.integrity)\n          res.size !== null && this.emit('size', res.size)\n          cb()\n        },\n        (er) => cb(er)\n      )\n    })\n  }\n}\n\nfunction writeStream (cache, opts = {}) {\n  return new CacacheWriteStream(cache, opts)\n}\n\nfunction handleContent (inputStream, cache, opts) {\n  return disposer(makeTmp(cache, opts), makeTmpDisposer, (tmp) => {\n    return pipeToTmp(inputStream, cache, tmp.target, opts)\n      .then((res) => {\n        return moveToDestination(\n          tmp,\n          cache,\n          res.integrity,\n          opts\n        ).then(() => res)\n      })\n  })\n}\n\nfunction pipeToTmp (inputStream, cache, tmpTarget, opts) {\n  let integrity\n  let size\n  const hashStream = ssri.integrityStream({\n    integrity: opts.integrity,\n    algorithms: opts.algorithms,\n    size: opts.size,\n  })\n  hashStream.on('integrity', i => {\n    integrity = i\n  })\n  hashStream.on('size', s => {\n    size = s\n  })\n\n  const outStream = new fsm.WriteStream(tmpTarget, {\n    flags: 'wx',\n  })\n\n  // NB: this can throw if the hashStream has a problem with\n  // it, and the data is fully written.  but pipeToTmp is only\n  // called in promisory contexts where that is handled.\n  const pipeline = new Pipeline(\n    inputStream,\n    hashStream,\n    outStream\n  )\n\n  return pipeline.promise()\n    .then(() => ({ integrity, size }))\n    .catch(er => rimraf(tmpTarget).then(() => {\n      throw er\n    }))\n}\n\nfunction makeTmp (cache, opts) {\n  const tmpTarget = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix)\n  return fixOwner.mkdirfix(cache, path.dirname(tmpTarget)).then(() => ({\n    target: tmpTarget,\n    moved: false,\n  }))\n}\n\nfunction makeTmpDisposer (tmp) {\n  if (tmp.moved)\n    return Promise.resolve()\n\n  return rimraf(tmp.target)\n}\n\nfunction moveToDestination (tmp, cache, sri, opts) {\n  const destination = contentPath(cache, sri)\n  const destDir = path.dirname(destination)\n\n  return fixOwner\n    .mkdirfix(cache, destDir)\n    .then(() => {\n      return moveFile(tmp.target, destination)\n    })\n    .then(() => {\n      tmp.moved = true\n      return fixOwner.chownr(cache, destination)\n    })\n}\n\nfunction sizeError (expected, found) {\n  const err = new Error(`Bad data size: expected inserted data to be ${expected} bytes, but got ${found} instead`)\n  err.expected = expected\n  err.found = found\n  err.code = 'EBADSIZE'\n  return err\n}\n\nfunction checksumError (expected, found) {\n  const err = new Error(`Integrity check failed:\n  Wanted: ${expected}\n   Found: ${found}`)\n  err.code = 'EINTEGRITY'\n  err.expected = expected\n  err.found = found\n  return err\n}\n","\n\nconst fs = require('fs')\nconst util = require('util')\nconst chmod = util.promisify(fs.chmod)\nconst unlink = util.promisify(fs.unlink)\nconst stat = util.promisify(fs.stat)\nconst move = require('@npmcli/move-file')\nconst pinflight = require('promise-inflight')\n\nmodule.exports = moveFile\n\nfunction moveFile (src, dest) {\n  const isWindows = global.__CACACHE_TEST_FAKE_WINDOWS__ ||\n    process.platform === 'win32'\n\n  // This isn't quite an fs.rename -- the assumption is that\n  // if `dest` already exists, and we get certain errors while\n  // trying to move it, we should just not bother.\n  //\n  // In the case of cache corruption, users will receive an\n  // EINTEGRITY error elsewhere, and can remove the offending\n  // content their own way.\n  //\n  // Note that, as the name suggests, this strictly only supports file moves.\n  return new Promise((resolve, reject) => {\n    fs.link(src, dest, (err) => {\n      if (err) {\n        if (isWindows && err.code === 'EPERM') {\n          // XXX This is a really weird way to handle this situation, as it\n          // results in the src file being deleted even though the dest\n          // might not exist.  Since we pretty much always write files to\n          // deterministic locations based on content hash, this is likely\n          // ok (or at worst, just ends in a future cache miss).  But it would\n          // be worth investigating at some time in the future if this is\n          // really what we want to do here.\n          return resolve()\n        } else if (err.code === 'EEXIST' || err.code === 'EBUSY') {\n          // file already exists, so whatever\n          return resolve()\n        } else\n          return reject(err)\n      } else\n        return resolve()\n    })\n  })\n    .then(() => {\n      // content should never change for any reason, so make it read-only\n      return Promise.all([\n        unlink(src),\n        !isWindows && chmod(dest, '0444'),\n      ])\n    })\n    .catch(() => {\n      return pinflight('cacache-move-file:' + dest, () => {\n        return stat(dest).catch((err) => {\n          if (err.code !== 'ENOENT') {\n            // Something else is wrong here. Bail bail bail\n            throw err\n          }\n          // file doesn't already exist! let's try a rename -> copy fallback\n          // only delete if it successfully copies\n          return move(src, dest)\n        })\n      })\n    })\n}\n","\n\nconst util = require('util')\n\nconst index = require('./lib/entry-index')\nconst memo = require('./lib/memoization')\nconst path = require('path')\nconst rimraf = util.promisify(require('rimraf'))\nconst rmContent = require('./lib/content/rm')\n\nmodule.exports = entry\nmodule.exports.entry = entry\n\nfunction entry (cache, key, opts) {\n  memo.clearMemoized()\n  return index.delete(cache, key, opts)\n}\n\nmodule.exports.content = content\n\nfunction content (cache, integrity) {\n  memo.clearMemoized()\n  return rmContent(cache, integrity)\n}\n\nmodule.exports.all = all\n\nfunction all (cache) {\n  memo.clearMemoized()\n  return rimraf(path.join(cache, '*(content-*|index-*)'))\n}\n","\n\nconst util = require('util')\n\nconst contentPath = require('./path')\nconst { hasContent } = require('./read')\nconst rimraf = util.promisify(require('rimraf'))\n\nmodule.exports = rm\n\nfunction rm (cache, integrity) {\n  return hasContent(cache, integrity).then((content) => {\n    // ~pretty~ sure we can't end up with a content lacking sri, but be safe\n    if (content && content.sri)\n      return rimraf(contentPath(cache, content.sri)).then(() => true)\n    else\n      return false\n  })\n}\n","\n\nmodule.exports = require('./lib/verify')\n","\n\nconst util = require('util')\n\nconst pMap = require('p-map')\nconst contentPath = require('./content/path')\nconst fixOwner = require('./util/fix-owner')\nconst fs = require('fs')\nconst fsm = require('fs-minipass')\nconst glob = util.promisify(require('glob'))\nconst index = require('./entry-index')\nconst path = require('path')\nconst rimraf = util.promisify(require('rimraf'))\nconst ssri = require('ssri')\n\nconst hasOwnProperty = (obj, key) =>\n  Object.prototype.hasOwnProperty.call(obj, key)\n\nconst stat = util.promisify(fs.stat)\nconst truncate = util.promisify(fs.truncate)\nconst writeFile = util.promisify(fs.writeFile)\nconst readFile = util.promisify(fs.readFile)\n\nconst verifyOpts = (opts) => ({\n  concurrency: 20,\n  log: { silly () {} },\n  ...opts,\n})\n\nmodule.exports = verify\n\nfunction verify (cache, opts) {\n  opts = verifyOpts(opts)\n  opts.log.silly('verify', 'verifying cache at', cache)\n\n  const steps = [\n    markStartTime,\n    fixPerms,\n    garbageCollect,\n    rebuildIndex,\n    cleanTmp,\n    writeVerifile,\n    markEndTime,\n  ]\n\n  return steps\n    .reduce((promise, step, i) => {\n      const label = step.name\n      const start = new Date()\n      return promise.then((stats) => {\n        return step(cache, opts).then((s) => {\n          s &&\n            Object.keys(s).forEach((k) => {\n              stats[k] = s[k]\n            })\n          const end = new Date()\n          if (!stats.runTime)\n            stats.runTime = {}\n\n          stats.runTime[label] = end - start\n          return Promise.resolve(stats)\n        })\n      })\n    }, Promise.resolve({}))\n    .then((stats) => {\n      stats.runTime.total = stats.endTime - stats.startTime\n      opts.log.silly(\n        'verify',\n        'verification finished for',\n        cache,\n        'in',\n        `${stats.runTime.total}ms`\n      )\n      return stats\n    })\n}\n\nfunction markStartTime (cache, opts) {\n  return Promise.resolve({ startTime: new Date() })\n}\n\nfunction markEndTime (cache, opts) {\n  return Promise.resolve({ endTime: new Date() })\n}\n\nfunction fixPerms (cache, opts) {\n  opts.log.silly('verify', 'fixing cache permissions')\n  return fixOwner\n    .mkdirfix(cache, cache)\n    .then(() => {\n      // TODO - fix file permissions too\n      return fixOwner.chownr(cache, cache)\n    })\n    .then(() => null)\n}\n\n// Implements a naive mark-and-sweep tracing garbage collector.\n//\n// The algorithm is basically as follows:\n// 1. Read (and filter) all index entries (\"pointers\")\n// 2. Mark each integrity value as \"live\"\n// 3. Read entire filesystem tree in `content-vX/` dir\n// 4. If content is live, verify its checksum and delete it if it fails\n// 5. If content is not marked as live, rimraf it.\n//\nfunction garbageCollect (cache, opts) {\n  opts.log.silly('verify', 'garbage collecting content')\n  const indexStream = index.lsStream(cache)\n  const liveContent = new Set()\n  indexStream.on('data', (entry) => {\n    if (opts.filter && !opts.filter(entry))\n      return\n\n    liveContent.add(entry.integrity.toString())\n  })\n  return new Promise((resolve, reject) => {\n    indexStream.on('end', resolve).on('error', reject)\n  }).then(() => {\n    const contentDir = contentPath.contentDir(cache)\n    return glob(path.join(contentDir, '**'), {\n      follow: false,\n      nodir: true,\n      nosort: true,\n    }).then((files) => {\n      return Promise.resolve({\n        verifiedContent: 0,\n        reclaimedCount: 0,\n        reclaimedSize: 0,\n        badContentCount: 0,\n        keptSize: 0,\n      }).then((stats) =>\n        pMap(\n          files,\n          (f) => {\n            const split = f.split(/[/\\\\]/)\n            const digest = split.slice(split.length - 3).join('')\n            const algo = split[split.length - 4]\n            const integrity = ssri.fromHex(digest, algo)\n            if (liveContent.has(integrity.toString())) {\n              return verifyContent(f, integrity).then((info) => {\n                if (!info.valid) {\n                  stats.reclaimedCount++\n                  stats.badContentCount++\n                  stats.reclaimedSize += info.size\n                } else {\n                  stats.verifiedContent++\n                  stats.keptSize += info.size\n                }\n                return stats\n              })\n            } else {\n              // No entries refer to this content. We can delete.\n              stats.reclaimedCount++\n              return stat(f).then((s) => {\n                return rimraf(f).then(() => {\n                  stats.reclaimedSize += s.size\n                  return stats\n                })\n              })\n            }\n          },\n          { concurrency: opts.concurrency }\n        ).then(() => stats)\n      )\n    })\n  })\n}\n\nfunction verifyContent (filepath, sri) {\n  return stat(filepath)\n    .then((s) => {\n      const contentInfo = {\n        size: s.size,\n        valid: true,\n      }\n      return ssri\n        .checkStream(new fsm.ReadStream(filepath), sri)\n        .catch((err) => {\n          if (err.code !== 'EINTEGRITY')\n            throw err\n\n          return rimraf(filepath).then(() => {\n            contentInfo.valid = false\n          })\n        })\n        .then(() => contentInfo)\n    })\n    .catch((err) => {\n      if (err.code === 'ENOENT')\n        return { size: 0, valid: false }\n\n      throw err\n    })\n}\n\nfunction rebuildIndex (cache, opts) {\n  opts.log.silly('verify', 'rebuilding index')\n  return index.ls(cache).then((entries) => {\n    const stats = {\n      missingContent: 0,\n      rejectedEntries: 0,\n      totalEntries: 0,\n    }\n    const buckets = {}\n    for (const k in entries) {\n      /* istanbul ignore else */\n      if (hasOwnProperty(entries, k)) {\n        const hashed = index.hashKey(k)\n        const entry = entries[k]\n        const excluded = opts.filter && !opts.filter(entry)\n        excluded && stats.rejectedEntries++\n        if (buckets[hashed] && !excluded)\n          buckets[hashed].push(entry)\n        else if (buckets[hashed] && excluded) {\n          // skip\n        } else if (excluded) {\n          buckets[hashed] = []\n          buckets[hashed]._path = index.bucketPath(cache, k)\n        } else {\n          buckets[hashed] = [entry]\n          buckets[hashed]._path = index.bucketPath(cache, k)\n        }\n      }\n    }\n    return pMap(\n      Object.keys(buckets),\n      (key) => {\n        return rebuildBucket(cache, buckets[key], stats, opts)\n      },\n      { concurrency: opts.concurrency }\n    ).then(() => stats)\n  })\n}\n\nfunction rebuildBucket (cache, bucket, stats, opts) {\n  return truncate(bucket._path).then(() => {\n    // This needs to be serialized because cacache explicitly\n    // lets very racy bucket conflicts clobber each other.\n    return bucket.reduce((promise, entry) => {\n      return promise.then(() => {\n        const content = contentPath(cache, entry.integrity)\n        return stat(content)\n          .then(() => {\n            return index\n              .insert(cache, entry.key, entry.integrity, {\n                metadata: entry.metadata,\n                size: entry.size,\n              })\n              .then(() => {\n                stats.totalEntries++\n              })\n          })\n          .catch((err) => {\n            if (err.code === 'ENOENT') {\n              stats.rejectedEntries++\n              stats.missingContent++\n              return\n            }\n            throw err\n          })\n      })\n    }, Promise.resolve())\n  })\n}\n\nfunction cleanTmp (cache, opts) {\n  opts.log.silly('verify', 'cleaning tmp directory')\n  return rimraf(path.join(cache, 'tmp'))\n}\n\nfunction writeVerifile (cache, opts) {\n  const verifile = path.join(cache, '_lastverified')\n  opts.log.silly('verify', 'writing verifile to ' + verifile)\n  try {\n    return writeFile(verifile, '' + +new Date())\n  } finally {\n    fixOwner.chownr.sync(cache, verifile)\n  }\n}\n\nmodule.exports.lastRun = lastRun\n\nfunction lastRun (cache) {\n  return readFile(path.join(cache, '_lastverified'), 'utf8').then(\n    (data) => new Date(+data)\n  )\n}\n","\n\nconst fs = require('@npmcli/fs')\n\nconst fixOwner = require('./fix-owner')\nconst path = require('path')\n\nmodule.exports.mkdir = mktmpdir\n\nfunction mktmpdir (cache, opts = {}) {\n  const { tmpPrefix } = opts\n  const tmpDir = path.join(cache, 'tmp')\n  return fs.mkdir(tmpDir, { recursive: true, owner: 'inherit' })\n    .then(() => {\n      // do not use path.join(), it drops the trailing / if tmpPrefix is unset\n      const target = `${tmpDir}${path.sep}${tmpPrefix || ''}`\n      return fs.mkdtemp(target, { owner: 'inherit' })\n    })\n}\n\nmodule.exports.withTmp = withTmp\n\nfunction withTmp (cache, opts, cb) {\n  if (!cb) {\n    cb = opts\n    opts = {}\n  }\n  return fs.withTempDir(path.join(cache, 'tmp'), cb, opts)\n}\n\nmodule.exports.fix = fixtmpdir\n\nfunction fixtmpdir (cache) {\n  return fixOwner(cache, path.join(cache, 'tmp'))\n}\n"]}